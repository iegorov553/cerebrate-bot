#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ö–∞—Ä–¥–∫–æ–¥–∞ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.
"""

import re
import json
from pathlib import Path
from typing import List, Dict, Tuple, Set

class HardcodeAnalyzer:
    def __init__(self):
        self.russian_patterns = [
            r'[–∞-—è—ë]',  # –†—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã
            r'[–ê-–Ø–Å]',  # –†—É—Å—Å–∫–∏–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        ]
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏—è - –Ω–µ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç
        self.exclusions = [
            r'import.*from.*',
            r'#.*',  # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            r'""".*"""',  # Docstrings
            r'logger\.',
            r'self\.',
            r'def\s+',
            r'class\s+',
            r'async\s+def',
            r'await\s+',
            r'return\s+',
            r'raise\s+',
            r'except\s+',
            r'try:',
            r'if\s+',
            r'elif\s+',
            r'else:',
            r'for\s+',
            r'while\s+',
            r'with\s+',
            r'assert\s+',
            r'print\(',
            r'\.format\(',
            r'\.translate\(',
            r'translator\.translate\(',
            r'f".*{.*}.*"',  # f-—Å—Ç—Ä–æ–∫–∏ —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            r"f'.*{.*}.*'",  # f-—Å—Ç—Ä–æ–∫–∏ —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏
            r'callback_data=',
            r'parse_mode=',
            r'encoding=',
            r'utf-8',
            r'markdown',
            r'html',
            r'json',
            r'\.py$',
            r'\.md$',
            r'\.txt$',
            r'\.json$',
            r'bot/',
            r'tests/',
            r'\.log',
            r'__pycache__',
            r'\.pyc',
            r'\.git',
            r'error=str\(e\)',
            r'user_id=',
            r'username=',
            r'callback_data=',
            r'file_path=',
            r'pattern=',
            r'path=',
            r'name=',
            r'text=',
            r'description=',
            r'content=',
            r'message=',
            r'data=',
            r'query=',
            r'context=',
            r'translator=',
            r'config=',
            r'client=',
            r'cache=',
            r'logger=',
            r'result=',
            r'response=',
            r'request=',
            r'session=',
            r'token=',
            r'key=',
            r'value=',
            r'status=',
            r'code=',
            r'url=',
            r'api=',
            r'db=',
            r'sql=',
            r'table=',
            r'column=',
            r'row=',
            r'field=',
            r'param=',
            r'arg=',
            r'kwargs=',
            r'args=',
            r'self\.',
            r'cls\.',
            r'super\(\)',
            r'__init__',
            r'__str__',
            r'__repr__',
            r'__len__',
            r'__iter__',
            r'__next__',
            r'__enter__',
            r'__exit__',
            r'__call__',
            r'__getitem__',
            r'__setitem__',
            r'__delitem__',
            r'__contains__',
            r'__eq__',
            r'__ne__',
            r'__lt__',
            r'__le__',
            r'__gt__',
            r'__ge__',
            r'__bool__',
            r'__hash__',
            r'__dict__',
            r'__class__',
            r'__name__',
            r'__file__',
            r'__path__',
            r'__package__',
            r'__version__',
            r'__author__',
            r'__email__',
            r'__license__',
            r'__copyright__',
            r'__credits__',
            r'__status__',
            r'__doc__',
            r'__annotations__',
            r'__module__',
            r'__qualname__',
            r'__slots__',
            r'__weakref__',
            r'__dict__',
            r'__getattribute__',
            r'__setattr__',
            r'__delattr__',
            r'__dir__',
            r'__sizeof__',
            r'__format__',
            r'__reduce__',
            r'__reduce_ex__',
            r'__getnewargs__',
            r'__getnewargs_ex__',
            r'__getstate__',
            r'__setstate__',
            r'__copy__',
            r'__deepcopy__',
            r'__pickle__',
            r'__unpickle__',
            r'True',
            r'False',
            r'None',
            r'NotImplemented',
            r'Ellipsis',
            r'__debug__',
            r'quit',
            r'exit',
            r'copyright',
            r'credits',
            r'license',
            r'help',
        ]
    
    def is_russian_text(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã."""
        for pattern in self.russian_patterns:
            if re.search(pattern, text):
                return True
        return False
    
    def is_excluded(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –¥–æ–ª–∂–Ω–∞ –ª–∏ —Å—Ç—Ä–æ–∫–∞ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞."""
        for pattern in self.exclusions:
            if re.search(pattern, line, re.IGNORECASE):
                return True
        return False
    
    def extract_hardcoded_strings(self, line: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–¥–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –∫–æ–¥–∞."""
        strings = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç—Ä–æ–∫
        patterns = [
            r'"([^"]*[–∞-—è—ë–ê-–Ø–Å][^"]*)"',  # –î–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏
            r"'([^']*[–∞-—è—ë–ê-–Ø–Å][^']*)'",  # –û–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏
            r'f"([^"]*[–∞-—è—ë–ê-–Ø–Å][^"]*)"',  # f-—Å—Ç—Ä–æ–∫–∏ —Å –¥–≤–æ–π–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
            r"f'([^']*[–∞-—è—ë–ê-–Ø–Å][^']*)'",  # f-—Å—Ç—Ä–æ–∫–∏ —Å –æ–¥–∏–Ω–∞—Ä–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, line)
            for match in matches:
                if self.is_russian_text(match) and not self.is_excluded(match):
                    strings.append(match)
        
        return strings
    
    def analyze_file(self, file_path: Path) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ö–∞—Ä–¥–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        violations = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if self.is_excluded(line):
                        continue
                    
                    hardcoded_strings = self.extract_hardcoded_strings(line)
                    for string in hardcoded_strings:
                        violations.append({
                            'file': str(file_path),
                            'line': line_num,
                            'text': string,
                            'full_line': line.strip(),
                            'severity': 'high' if len(string) > 10 else 'medium'
                        })
        
        except (UnicodeDecodeError, PermissionError):
            pass
        
        return violations
    
    def analyze_project(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–µ—Å—å –ø—Ä–æ–µ–∫—Ç."""
        result = {
            'total_violations': 0,
            'files_with_violations': 0,
            'violations_by_file': {},
            'unique_strings': set(),
            'severity_counts': {'high': 0, 'medium': 0, 'low': 0}
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        patterns = [
            'bot/handlers/**/*.py',
            'bot/keyboards/**/*.py',
            'bot/utils/**/*.py',
            'bot/services/**/*.py',
            'bot/database/**/*.py',
            'bot/admin/**/*.py',
            'bot/cache/**/*.py',
            'bot/questions/**/*.py',
            'bot/feedback/**/*.py',
            'bot/i18n/**/*.py',
        ]
        
        for pattern in patterns:
            for file_path in Path('.').glob(pattern):
                if file_path.is_file():
                    violations = self.analyze_file(file_path)
                    if violations:
                        result['files_with_violations'] += 1
                        result['violations_by_file'][str(file_path)] = violations
                        result['total_violations'] += len(violations)
                        
                        for violation in violations:
                            result['unique_strings'].add(violation['text'])
                            result['severity_counts'][violation['severity']] += 1
        
        return result
    
    def generate_mapping(self, violations: Dict) -> Dict[str, str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç mapping –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è."""
        mapping = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ –ø–æ —Ç–∏–ø–∞–º
        for file_path, file_violations in violations['violations_by_file'].items():
            for violation in file_violations:
                text = violation['text']
                if text not in mapping:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                    key = self.generate_translation_key(text, file_path)
                    mapping[text] = key
        
        return mapping
    
    def generate_translation_key(self, text: str, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ –∏ —Ñ–∞–π–ª–∞."""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —Ñ–∞–π–ª—É
        if 'admin' in file_path:
            category = 'admin'
        elif 'friend' in file_path:
            category = 'friends'
        elif 'config' in file_path:
            category = 'config'
        elif 'error' in file_path:
            category = 'errors'
        elif 'broadcast' in file_path:
            category = 'broadcast'
        elif 'question' in file_path:
            category = 'questions'
        else:
            category = 'common'
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        key_part = text.lower()
        key_part = re.sub(r'[^–∞-—è—ëa-z0-9\s]', '', key_part)
        key_part = re.sub(r'\s+', '_', key_part)
        key_part = key_part[:30]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è
        if text.startswith('‚ùå'):
            type_prefix = 'error'
        elif text.startswith('‚úÖ'):
            type_prefix = 'success'
        elif text.startswith('‚ö†Ô∏è'):
            type_prefix = 'warning'
        elif text.startswith('üì¢'):
            type_prefix = 'broadcast'
        elif text.startswith('üì•'):
            type_prefix = 'loading'
        elif text.startswith('‚è∞'):
            type_prefix = 'time'
        elif text.startswith('üîí'):
            type_prefix = 'access'
        elif text.startswith('üìä'):
            type_prefix = 'stats'
        elif '**' in text:
            type_prefix = 'title'
        elif '–ü—Ä–∏–º–µ—Ä:' in text:
            type_prefix = 'example'
        elif '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:' in text:
            type_prefix = 'usage'
        else:
            type_prefix = 'message'
        
        return f"{category}.{type_prefix}_{key_part}"
    
    def save_results(self, violations: Dict, output_file: str = "hardcode_analysis.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞."""
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON
        violations_copy = violations.copy()
        violations_copy['unique_strings'] = list(violations['unique_strings'])
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(violations_copy, f, ensure_ascii=False, indent=2)
    
    def print_summary(self, violations: Dict):
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É."""
        print(f"üìä –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"üìÅ –§–∞–π–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏: {violations['files_with_violations']}")
        print(f"üîç –í—Å–µ–≥–æ –Ω–∞—Ä—É—à–µ–Ω–∏–π: {violations['total_violations']}")
        print(f"üìù –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(violations['unique_strings'])}")
        print(f"üî¥ –í—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å: {violations['severity_counts']['high']}")
        print(f"üü° –°—Ä–µ–¥–Ω—è—è –≤–∞–∂–Ω–æ—Å—Ç—å: {violations['severity_counts']['medium']}")
        print(f"üü¢ –ù–∏–∑–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å: {violations['severity_counts']['low']}")
        
        print("\nüìã –¢–æ–ø –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
        sorted_files = sorted(violations['violations_by_file'].items(), 
                            key=lambda x: len(x[1]), reverse=True)
        for file_path, file_violations in sorted_files[:10]:
            print(f"  {file_path}: {len(file_violations)} –Ω–∞—Ä—É—à–µ–Ω–∏–π")
        
        print("\nüéØ –ü—Ä–∏–º–µ—Ä—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫:")
        for i, string in enumerate(list(violations['unique_strings'])[:10]):
            print(f"  {i+1}. \"{string}\"")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    analyzer = HardcodeAnalyzer()
    
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ö–∞—Ä–¥–∫–æ–¥ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
    violations = analyzer.analyze_project()
    
    analyzer.print_summary(violations)
    analyzer.save_results(violations)
    
    print(f"\nüìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ hardcode_analysis.json")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º mapping
    mapping = analyzer.generate_mapping(violations)
    with open('hardcode_mapping.json', 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    
    print(f"üìÑ Mapping —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ hardcode_mapping.json")

if __name__ == "__main__":
    main()